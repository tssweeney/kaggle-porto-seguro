{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Helper-Functions\" data-toc-modified-id=\"Helper-Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Helper Functions</a></span></li><li><span><a href=\"#Data-Management\" data-toc-modified-id=\"Data-Management-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Management</a></span></li><li><span><a href=\"#Data-Cleaning\" data-toc-modified-id=\"Data-Cleaning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Cleaning</a></span></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Engineering</a></span></li><li><span><a href=\"#Dimension-Reduction\" data-toc-modified-id=\"Dimension-Reduction-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Dimension Reduction</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Model Training</a></span></li><li><span><a href=\"#Kaggle-Prediction\" data-toc-modified-id=\"Kaggle-Prediction-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Kaggle Prediction</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Global Data Science Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Model Settings\n",
    "SETTINGS = {\n",
    "    # VERSION indicates the model version\n",
    "    \"VERSION\": 9,\n",
    "    \n",
    "    # TARGET_POSITIVE_PERCENT determines the desired mix of positive labels in the training set.\n",
    "    # Since the raw data only contains 5% positive values, this can be beneficial in two ways:\n",
    "    # 1) Reduces the data set\n",
    "    # 2) Balances the positive outcomes\n",
    "    \"TARGET_POSITIVE_PERCENT\": 0.15,\n",
    "    \n",
    "    # Indicates the cross_validation count. Mostly this is just used to manage runtime.\n",
    "    \"SPLIT_COUNT\": 3,\n",
    "    \n",
    "    # Shall we actually save submission, or are we testing the pipeline?\n",
    "    \"SUBMITTING\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# The logger class is used to save logs in an ordered manner\n",
    "# After instanciation, call #.log(string) to both print a string as\n",
    "# well as save to a master string. This master string will be used\n",
    "# by the final submission function to save the log data.\n",
    "\n",
    "class Logger():\n",
    "    def __init__(self):\n",
    "        self.log_str = \"\"\n",
    "    \n",
    "    def log(self, string):\n",
    "        print(string)\n",
    "        date = datetime.datetime.now()\n",
    "        date_str = str(date.hour) + \"::\" + str(date.minute) + \"::\" + str(date.second)\n",
    "        string = \"\\n(\"+date_str+\")\\n\" + string \n",
    "        self.log_str = self.log_str + string\n",
    "        \n",
    "    def get_log(self):\n",
    "        return self.log_str\n",
    "\n",
    "# Instanciate a global Logger: L\n",
    "L = Logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "# record_attempt is used at the end of the pipeline to do the following:\n",
    "# 1) Setup a new timestamped directory\n",
    "# 2) Save the master Log (we are referencing the global logger - bad programming!)\n",
    "# 3) Pickle and save the submissions and the pipeline\n",
    "# 4) Save CSVs of the final submission.\n",
    "\n",
    "def record_attempt(columns, df_submission, pipeline):\n",
    "    assert(df_submission.shape[0] == 892816)\n",
    "    assert(df_submission.columns[0] == \"id\")\n",
    "    \n",
    "    date = datetime.datetime.now()\n",
    "    date_str = str(date.year) + \"_\" + str(date.month) + \"_\" + str(date.day) + \"_\" + str(date.hour) + \"_\" + str(date.minute) + \"_\" + str(date.second)\n",
    "    \n",
    "    # Make directory:\n",
    "    directory = \"../submissions/\" + date_str + \"/\"\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "    file = open(directory + \"details__\" +date_str+ \".txt\", \"w\")\n",
    "    file.write(\"\\n\\nSETTINGS:\")\n",
    "    for key in SETTINGS:\n",
    "        file.write(\"\\n\\t\" + str(key) + \" : \" + str(SETTINGS[key]))\n",
    "        \n",
    "    file.write(\"\\n\\nPIPELINE:\")\n",
    "    file.write(\"\\n\" + str(pprint.pformat(pipeline)))\n",
    "        \n",
    "    if (L):\n",
    "        file.write(\"\\n\\n\" + L.get_log())\n",
    "        \n",
    "    file.close()\n",
    "    \n",
    "    for col in columns: #df_submission.columns :\n",
    "        if col != \"id\":\n",
    "            df_submission[[\"id\"] + [col]].to_csv(directory + \"model_\" + col + \"__\" +date_str+ \".csv\", header=[\"id\", \"target\"], index=False)\n",
    "    \n",
    "    pd.to_pickle(pipeline, directory + \"pipeline__\" +date_str+ \".pkl\")\n",
    "    pd.to_pickle(df_submission, directory + \"predictions__\" +date_str+ \".pkl\")\n",
    "    \n",
    "    print(\"Recorded to \" + directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# gini and gini_normalized are used to score various models. These functions were given by Kaggle Competition. \n",
    "# gini_normalized will output a value between 0.0 - 0.5. This is the same score you get with kaggle.\n",
    "# at the end we create a gini_scorer which can be used by sklearn estimators.\n",
    "\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "\n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    "\n",
    "def gini_normalized(a, p):\n",
    "     return gini(a, p) / gini(a, a)\n",
    "    \n",
    "gini_scorer = make_scorer(gini_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_submit = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# TransformerMixin is used to create pipeline steps\n",
    "# Note, my Transformers rely on the data being in dataframes, and therefore have column names\n",
    "# These CANNOT be used in the stanard sklearn Pipeline. I may refactor this if I have time.\n",
    "\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# NumericImputer conducts 2 transformations:\n",
    "# 1) Creates a new columns \"[feature]_filled\" indicating that the original feature was not null\n",
    "# 2) Performs a simple mean imputation on the column\n",
    "\n",
    "class NumericImputer(TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.imputers = {}\n",
    "        for col in self.columns:\n",
    "            self.imputers[col] = Imputer(missing_values=-1).fit(X[col].reshape(-1, 1))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        df_return = X.copy()\n",
    "        for col in self.columns:\n",
    "            df_return[col + \"_filled\"] = np.logical_not(df_return[col] == -1)\n",
    "            df_return[col] = self.imputers[col].transform(X[col].reshape(-1, 1))\n",
    "                \n",
    "        return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# CategoricalEncoder performs basically dummy-encodes categorical values\n",
    "# I found a bug in the LabelBinizer which made this a bit messy.\n",
    "\n",
    "class CategoricalEncoder(TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.encoders = {}\n",
    "        for col in self.columns:\n",
    "            self.encoders[col] = LabelBinarizer().fit(X[col])\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        df_return = X.copy()\n",
    "        for col in self.columns:\n",
    "            encodings = self.encoders[col].transform(X[col]).T\n",
    "            # found bug in LabelBinarizer\n",
    "            if encodings.shape[0] != 1:\n",
    "                encodings = encodings[1:]\n",
    "            class_ndx = 1\n",
    "            for encoding in encodings:\n",
    "                df_return[col + \"_\" + str(self.encoders[col].classes_[class_ndx])] = encoding\n",
    "                class_ndx = class_ndx + 1\n",
    "                \n",
    "        return df_return[[col for col in df_return.columns if col not in self.columns]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# CustomPoly is a PolynomialFeatures wrapper that works with DataFrame (this is probably overkill)\n",
    "\n",
    "class CustomPoly(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.poly = PolynomialFeatures(interaction_only=True).fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        df_return = pd.DataFrame(self.poly.transform(X))\n",
    "        return df_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# CustomPCA is a PCA wrapper that works with DataFrames and allows the researcher to specify the minimum explained_variance_ratio\n",
    "\n",
    "class CustomPCA(TransformerMixin):\n",
    "    def __init__(self, variance_threshold = 0.01):\n",
    "        self.variance_threshold = variance_threshold\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.pca = PCA().fit(X)\n",
    "        self.num_cols = (self.pca.explained_variance_ratio_ > self.variance_threshold).sum()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        df_return = pd.DataFrame(self.pca.transform(X))[np.arange(0, self.num_cols + 1)]\n",
    "        return df_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standard Scaler wrapper...\n",
    "\n",
    "class CustomScaler(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler = StandardScaler().fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return pd.DataFrame(self.scaler.transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# This cell block results in a dataframe: df_working\n",
    "# df_working will contain all the positivily labeled examples as well as\n",
    "# as many random negatively labeled values to attain TARGET_POSITIVE_PERCENT\n",
    "\n",
    "num_positives = df_train[\"target\"].sum()\n",
    "percent_positive = num_positives / df_train.shape[0]\n",
    "desired_negatives = num_positives * (1 - SETTINGS[\"TARGET_POSITIVE_PERCENT\"]) / (SETTINGS[\"TARGET_POSITIVE_PERCENT\"] + 0.00001)\n",
    "additional_percent = desired_negatives / (1 - percent_positive) / df_train.shape[0]\n",
    "additional_ndxs = np.random.random(df_train.shape[0]) <= additional_percent\n",
    "\n",
    "working_ndxs = df_train[\"target\"] == 1\n",
    "working_ndxs = np.logical_or(working_ndxs, additional_ndxs)\n",
    "df_working = df_train[working_ndxs].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Now create list holding for each column type \n",
    "\n",
    "bin_cols = [col for col in df_working.columns if col[-3:] == \"bin\"]\n",
    "cat_cols = [col for col in df_working.columns if col[-3:] == \"cat\"]\n",
    "num_cols = [col for col in df_working.columns if col not in [\"id\", \"target\", \"index\"] + bin_cols + cat_cols]\n",
    "feature_cols = bin_cols + cat_cols + num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# And validate\n",
    "\n",
    "L.log(\"Total # of working observations: \" + str(df_working.shape[0]))\n",
    "L.log(\"Count of features: \" + str(len(feature_cols)))\n",
    "L.log(\"Percent of working data with target: \" + str(100 * df_working[\"target\"].sum() / df_working.shape[0]) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create a \"pipeline\" - but not sklearn pipeline. A customing hacky pipeline.\n",
    "\n",
    "pipeline = {\n",
    "    \"transformers\": [\n",
    "        (\"NumericImputer\", NumericImputer(columns = num_cols)),\n",
    "        (\"CategoricalEncoder\", CategoricalEncoder(columns = cat_cols + bin_cols)),\n",
    "        (\"Scaler\", CustomScaler()),\n",
    "#         (\"PolyNomialExpansion\", CustomPoly()),\n",
    "        (\"PCA\", CustomPCA(variance_threshold = 0.0001))\n",
    "    ],\n",
    "    \"models\": [],\n",
    "    \"scorers\": [\n",
    "        (\"Gini\", gini_normalized),\n",
    "        (\"MSE\", mean_squared_error)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Setup a load of classifiers\n",
    "\n",
    "TREE_DEPTH = 7\n",
    "NUM_ESTIMATORS = 50\n",
    "MAX_FEATURES = 'auto'\n",
    "OOB_SCORE = True\n",
    "VERBOSITY = True\n",
    "N_JOBS = 3\n",
    "RANDOM_STATE = 777\n",
    "WARM_START = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline[\"models\"].append((\"ExtraTreeClassifier\", ExtraTreeClassifier(\n",
    "    criterion='gini',\n",
    "    splitter='random',\n",
    "    max_depth=TREE_DEPTH,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=MAX_FEATURES,\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    class_weight=None\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline[\"models\"].append((\"DecisionTreeClassifier\", DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    splitter='best',\n",
    "    max_depth=TREE_DEPTH,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=MAX_FEATURES,\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    class_weight=None,\n",
    "    presort=False\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline[\"models\"].append((\"MLPClassifier\", MLPClassifier(\n",
    "    hidden_layer_sizes=(32,8,2),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=0.001,\n",
    "    power_t=0.5,\n",
    "    max_iter=200,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_STATE,\n",
    "    tol=0.0001,\n",
    "    verbose=VERBOSITY,\n",
    "    warm_start=WARM_START,\n",
    "    momentum=0.9,\n",
    "    nesterovs_momentum=True,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-08\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline[\"models\"].append((\"GaussianNB\", GaussianNB(\n",
    "    priors=None\n",
    ")))\n",
    "\n",
    "pipeline[\"models\"].append((\"BernoulliNB\", BernoulliNB(\n",
    "    alpha=1.0,\n",
    "    binarize=0.0,\n",
    "    fit_prior=True,\n",
    "    class_prior=None\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline[\"models\"].append((\"SGDClassifier\", SGDClassifier(\n",
    "    loss='hinge',\n",
    "    penalty='elasticnet',\n",
    "    alpha=0.0001,\n",
    "    l1_ratio=0.15,\n",
    "    fit_intercept=True,\n",
    "    max_iter=None,\n",
    "    tol=None,\n",
    "    shuffle=True,\n",
    "    verbose=VERBOSITY,\n",
    "    epsilon=0.1,\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    learning_rate='optimal',\n",
    "    eta0=0.0,\n",
    "    power_t=0.5,\n",
    "    class_weight=None,\n",
    "    warm_start=WARM_START,\n",
    "    average=False,\n",
    "    n_iter=None\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline[\"models\"].append((\"RidgeClassifier\", RidgeClassifier(\n",
    "    alpha=1.0,\n",
    "    fit_intercept=True,\n",
    "    normalize=False,\n",
    "    copy_X=True,\n",
    "    max_iter=None,\n",
    "    tol=0.001,\n",
    "    class_weight=None,\n",
    "    solver='auto',\n",
    "    random_state=RANDOM_STATE\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline[\"models\"].append((\"PassiveAggressiveClassifier\", PassiveAggressiveClassifier(\n",
    "    C=1.0,\n",
    "    fit_intercept=True,\n",
    "    max_iter=None,\n",
    "    tol=None,\n",
    "    shuffle=True,\n",
    "    verbose=VERBOSITY,\n",
    "    loss='hinge',\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    warm_start=WARM_START,\n",
    "    class_weight=None,\n",
    "    average=False,\n",
    "    n_iter=None\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline[\"models\"].append((\"RandomForestClassifier\", RandomForestClassifier(\n",
    "    n_estimators=NUM_ESTIMATORS,\n",
    "    criterion='gini',\n",
    "    max_depth=TREE_DEPTH,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=MAX_FEATURES,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=OOB_SCORE,\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=VERBOSITY,\n",
    "    warm_start=WARM_START,\n",
    "    class_weight=None\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline[\"models\"].append((\"GradientBoostingClassifier\", GradientBoostingClassifier(\n",
    "    loss='deviance', # Choosing 'deviance' as the alternative ('exponential' is effectivey Ada)\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=NUM_ESTIMATORS,\n",
    "    subsample=1.0,\n",
    "    criterion='friedman_mse',\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_depth=TREE_DEPTH,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    init=None,\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_features=MAX_FEATURES,\n",
    "    verbose=VERBOSITY,\n",
    "    max_leaf_nodes=None,\n",
    "    warm_start=WARM_START,\n",
    "    presort='auto'\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline[\"models\"].append((\"ExtraTreesClassifier\", ExtraTreesClassifier(\n",
    "    n_estimators=NUM_ESTIMATORS,\n",
    "    criterion='gini',\n",
    "    max_depth=TREE_DEPTH,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=MAX_FEATURES,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True, # changed from False to allow oob\n",
    "    oob_score=OOB_SCORE,\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=VERBOSITY,\n",
    "    warm_start=WARM_START,\n",
    "    class_weight=None\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# pipeline[\"models\"].append((\"AdaBoostClassifier\", AdaBoostClassifier(\n",
    "#     base_estimator=None,\n",
    "#     n_estimators=NUM_ESTIMATORS,\n",
    "#     learning_rate=1.0,\n",
    "#     algorithm='SAMME.R',\n",
    "#     random_state=RANDOM_STATE\n",
    "# )))\n",
    "\n",
    "# pipeline[\"models\"].append((\"RadiusNeighborsClassifier\", RadiusNeighborsClassifier(\n",
    "#     radius=1.0,\n",
    "#     weights='uniform',\n",
    "#     algorithm='auto',\n",
    "#     leaf_size=30,\n",
    "#     p=2,\n",
    "#     metric='minkowski',\n",
    "#     outlier_label=None,\n",
    "#     metric_params=None\n",
    "# )))\n",
    "# pipeline[\"models\"].append((\"KNeighborsClassifier\", KNeighborsClassifier(\n",
    "#     n_neighbors=5,\n",
    "#     weights='uniform',\n",
    "#     algorithm='auto',\n",
    "#     leaf_size=30,\n",
    "#     p=2,\n",
    "#     metric='minkowski',\n",
    "#     metric_params=None,\n",
    "#     n_jobs=N_JOBS\n",
    "# )))\n",
    "# pipeline[\"models\"].append((\"MultinomialNB\", MultinomialNB(\n",
    "#     alpha=1.0,\n",
    "#     fit_prior=True,\n",
    "#     class_prior=None\n",
    "# )))\n",
    "\n",
    "# pipeline[\"models\"].append((\"GaussianProcessClassifier\", GaussianProcessClassifier(\n",
    "#     kernel=None,\n",
    "#     optimizer='fmin_l_bfgs_b',\n",
    "#     n_restarts_optimizer=0,\n",
    "#     max_iter_predict=100,\n",
    "#     warm_start=WARM_START,\n",
    "#     copy_X_train=True,\n",
    "#     random_state=RANDOM_STATE,\n",
    "#     multi_class='one_vs_rest',\n",
    "#     n_jobs=N_JOBS\n",
    "# )))\n",
    "\n",
    "# pipeline[\"models\"].append((\"BaggingClassifier\", BaggingClassifier(\n",
    "#     base_estimator=None,\n",
    "#     n_estimators=NUM_ESTIMATORS,\n",
    "#     max_samples=1.0,\n",
    "#     max_features=1.0,\n",
    "#     bootstrap=True,\n",
    "#     bootstrap_features=False,\n",
    "#     oob_score=OOB_SCORE,\n",
    "#     warm_start=WARM_START,\n",
    "#     n_jobs=N_JOBS,\n",
    "#     random_state=RANDOM_STATE,\n",
    "#     verbose=VERBOSITY\n",
    "# )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import copy\n",
    "\n",
    "# Setup a few dataframes:\n",
    "# predictions - used to hold all our different classifier predictions\n",
    "# test_values -> the ultimately will be a new dataset of each model's prediction along with a target column\n",
    "\n",
    "predictions = pd.DataFrame(data = df_submit[\"id\"].tolist(), columns=[\"id\"])\n",
    "test_values = pd.DataFrame(index = df_working[\"id\"].tolist(), columns=[model[0] for model in pipeline[\"models\"]] + [\"target\"])\n",
    "test_values[\"target\"][df_working[\"id\"]] = df_working[\"target\"]\n",
    "\n",
    "# get_prediction_helper strips the probability from each classifier / regression\n",
    "\n",
    "def get_prediction_helper(estimator, X):\n",
    "    if hasattr(estimator, \"decision_function\"):\n",
    "        Y = estimator.decision_function(X)\n",
    "    elif hasattr(estimator, \"predict_proba\"):\n",
    "        Y = estimator.predict_proba(X).T[1]\n",
    "    else:\n",
    "        Y = estimator.predict(X)\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc \n",
    "\n",
    "def plot_roc(clf, X_train, Y_train, Y_train_pred, X_test, Y_test, Y_test_pred):\n",
    "        \n",
    "    train_fpr, train_tpr, train_thresholds = roc_curve(Y_train, Y_train_pred)\n",
    "    train_auc_score = auc(train_fpr, train_tpr)\n",
    "    \n",
    "    \n",
    "    test_fpr, test_tpr, test_thresholds = roc_curve(Y_test,Y_test_pred)\n",
    "    test_auc_score = auc(test_fpr, test_tpr)\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    lw = 2\n",
    "    plt.plot(train_fpr, train_tpr, color='darkorange',\n",
    "             lw=lw, label='Train ROC curve (area = %0.2f)' % train_auc_score)\n",
    "    plt.plot(test_fpr, test_tpr, color='darkred',\n",
    "             lw=lw, label='Test ROC curve (area = %0.2f)' % test_auc_score)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xticks(np.arange(0.0, 1.05, 0.1))\n",
    "    plt.yticks(np.arange(0.0, 1.05, 0.1))\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup a k-fold loop\n",
    "def train():\n",
    "    kf = KFold(n_splits=SETTINGS[\"SPLIT_COUNT\"], shuffle=True)\n",
    "\n",
    "    # some helpers\n",
    "\n",
    "    iteration = 0\n",
    "    models = []\n",
    "    transformers = []\n",
    "\n",
    "    # basically the meat:\n",
    "\n",
    "    for train_index, test_index in kf.split(df_working):\n",
    "        models.append(copy.deepcopy(pipeline[\"models\"]))\n",
    "        transformers.append(copy.deepcopy(pipeline[\"transformers\"]))\n",
    "\n",
    "        print(\"KFold iteration: \" + str(iteration))\n",
    "        train_X = df_working[feature_cols].iloc[train_index]\n",
    "        train_Y = df_working[\"target\"].iloc[train_index]\n",
    "        test_X = df_working[feature_cols].iloc[test_index]\n",
    "        test_Y = df_working[\"target\"].iloc[test_index]\n",
    "        test_ids = df_working[\"id\"].iloc[test_index]\n",
    "        submit_X = df_submit[feature_cols]\n",
    "\n",
    "        for transformer in transformers[iteration]:\n",
    "            print(\"\\tTransformer: \" + transformer[0])\n",
    "            print(\"\\t\\tFitting\")\n",
    "            transformer[1].fit(train_X)\n",
    "            print(\"\\t\\tTransforming Train\")\n",
    "            train_X = transformer[1].transform(train_X)\n",
    "            print(\"\\t\\tTransforming Test\")\n",
    "            test_X = transformer[1].transform(test_X)\n",
    "            if SETTINGS[\"SUBMITTING\"]:\n",
    "                print(\"\\t\\tTransforming Submission\")\n",
    "                submit_X = transformer[1].transform(submit_X)\n",
    "\n",
    "        for model in models[iteration]:\n",
    "            model_name = model[0] + \"_\" + str(iteration)\n",
    "\n",
    "            L.log(\"\\tModel: \" + model_name)\n",
    "            print(\"\\t\\tFitting\")\n",
    "            model[1].fit(train_X, train_Y)\n",
    "            print(\"\\t\\tPredicting Train\")\n",
    "            train_pred_Y = get_prediction_helper(model[1], train_X)\n",
    "            for scorer in pipeline[\"scorers\"]:\n",
    "                L.log(\"\\t\\t\\tTrain - \" + scorer[0] + \": \" + str(scorer[1](train_Y, train_pred_Y)))\n",
    "            print(\"\\t\\tPredicting Test\")\n",
    "            test_pred_Y = get_prediction_helper(model[1], test_X)\n",
    "            for scorer in pipeline[\"scorers\"]:\n",
    "                L.log(\"\\t\\t\\tTest - \" + scorer[0] + \": \" + str(scorer[1](test_Y, test_pred_Y)))\n",
    "            test_values[model[0]][test_ids] = MinMaxScaler().fit_transform(test_pred_Y.reshape(-1, 1)).T[0].tolist()\n",
    "\n",
    "            plot_roc(model[1], train_X, train_Y, train_pred_Y, test_X, test_Y, test_pred_Y)\n",
    "\n",
    "            if SETTINGS[\"SUBMITTING\"]:\n",
    "                print(\"\\t\\tPredicting Submission\")\n",
    "                submit_Y = MinMaxScaler().fit_transform(get_prediction_helper(model[1], submit_X).reshape(-1, 1))\n",
    "                prediction_attempt = submit_Y * gini_normalized(test_Y, test_pred_Y)\n",
    "                predictions[model_name] = prediction_attempt\n",
    "\n",
    "        iteration = iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# At this point, we have a table -> test values that holds the predicted values for each model (when testing) and a target. Time to get Meta!!\n",
    "\n",
    "# Now we traing a simple classifier on top of this hyper table\n",
    "\n",
    "clf = RidgeClassifier(class_weight=\"balanced\")\n",
    "\n",
    "\n",
    "meta_columns = [col for col in test_values.columns if col != \"target\"]\n",
    "X = test_values[meta_columns]\n",
    "Y = test_values[\"target\"]\n",
    "\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(X, Y, test_size=0.33)\n",
    "L.log(\"Training Meta Model\")\n",
    "clf.fit(X_train, Y_train)\n",
    "probs = get_prediction_helper(clf, X_test)\n",
    "probs = MinMaxScaler().fit_transform(probs.reshape(-1, 1)).T[0].tolist()\n",
    "gini_score = gini_normalized(Y_test, probs)\n",
    "L.log(\"\\tGini - \" + str(gini_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# If we are submitting, do the same for the prediction table\n",
    "\n",
    "if SETTINGS[\"SUBMITTING\"]:\n",
    "    meta_model = pd.DataFrame(data = df_submit[\"id\"].tolist(), columns=[\"id\"])\n",
    "    for meta in meta_columns:\n",
    "        meta_model[meta] = 0\n",
    "        for num in range(0, SETTINGS[\"SPLIT_COUNT\"]):\n",
    "            raw_label = meta + \"_\" + str(num)\n",
    "            meta_model[meta] = meta_model[meta].add(predictions[raw_label])\n",
    "            \n",
    "    meta_model[meta_columns] = meta_model[meta_columns] / (SETTINGS[\"SPLIT_COUNT\"] - 1)\n",
    "    predictions[\"mean_prediction\"] = MinMaxScaler(feature_range=(0.00001, 0.99999)).fit_transform(predictions[[col for col in predictions.columns if col != \"id\"]].sum(axis=1).reshape(-1, 1))\n",
    "    X = meta_model[meta_columns]\n",
    "    probs = get_prediction_helper(clf, X)\n",
    "    predictions[\"meta_prediction\"] = MinMaxScaler(feature_range=(0.00001, 0.99999)).fit_transform(probs.reshape(-1, 1))\n",
    "    \n",
    "    predictions.meta_prediction.sort_values(inplace=False).reset_index().meta_prediction.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kaggle Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Record the attempt\n",
    "\n",
    "record_attempt([\"mean_prediction\", \"meta_prediction\", \"RandomForestClassifier_0\"], predictions, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the data\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {
    "height": "298px",
    "width": "290px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
